<!DOCTYPE HTML>
<!--
	Arcana by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<style>
div.ml1 {
  margin-left: 30px;
}
ul.a {list-style-type: circle;}
ul.b {list-style-type: square;}
ul.c {list-style-type: square;}
</style>
<html>
	<head>
		<title>Challenge-Task1 Software</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
	</head>
	<body class="is-preload">
		<div id="page-wrapper">

			<!-- Header -->
				<div id="header">

					<!-- Logo -->
					<!-- <img src="images/isca.png" width="6%" height="6%" style="margin:0 77% 0 0;"> -->
					<!-- <h1 style="font-size: 120%;  margin: -4% 0 -2% 0"><a href="index.html" id="logo">Multimodal Information
							Based Speech Processing (MISP) Challenge 2022</a></h1> -->
					<h1 style="font-size: 120%;  margin: -1% 0% -1% 0%;"><a href="index.html" id="logo">Multimodal Information
								Based Speech Processing (MISP) Challenge 2022</a></h1> 	
					<img src="images/ieee.svg" width="7%"height="7%" style="margin:-10% 0% 3% 79%;">
					<!-- Nav -->
						<nav id="nav">
							<ul>
								<li><a href="index.html">Home</a></li>
								<li><a href="overview.html">Overview</a></li>
								<li><a href="data.html">Data</a></li>
								<li class="current">
									<a href="#">Task</a>
									<ul>
										<li><a href="task1_instructions.html">Instructions</a></li>										
										<li><a href="task1_software.html">Software</a></li>
						                <li><a href="task1_submission.html">Submission</a></li>
									</ul>
								</li>
								
								<!-- <li>
									<a href="#">Task3</a>
									<ul>
										<li><a href="task3_instructions.html">Instructions</a></li>
										<li><a href="task3_data.html">Data</a></li>
										<li><a href="task3_software.html">Software</a></li>
										<li><a href="task3_submission.html">Submission</a></li>
									</ul>
								</li> -->
								<li><a href="download.html">Registration</a></li>
								<li><a href="extral_data.html">Extra Data</a></li>
								<!-- <li><a href="results.html">Results</a></li> -->
								<!-- <li><a href="contact.html">Contact</a></li> -->
							</ul>
						</nav>

				</div>

			<!-- Main -->
			<h1 style="font-size: 150%;">Coming Soon...</h1>
			<!-- <section class="wrapper style1">
				<div class="container">
					<h1 style="font-size: 150%;">Track1 AVSD Baseline</h1>
					<p>For details, please refer to <a href="https://github.com/mispchallenge/misp2022_baseline/tree/main/track1_AVSD">Github link</a></p>				   

					<h1 style="font-size: 150%;">Data preparation</h1>

					<ul>
						<li><strong>Speech enhancement</strong></li>
						<p>Weighted Prediction Error(WPE) dereverberation is used to reduce the reverberations of speech signals. The algorithms is implemented with open-source toolkit, <a href="https://github.com/fgnt/nara_wpe">nara_wpe</a></p>

						<li><strong>Lip ROI extraction</strong></li>
						<p>In the dataset, we provide the position detection results of the face and lips, which can be used to extract the ROI of the face or lips (only the lips are used here). The extracted lip ROI size is 96 × 96 (W × H) .</p>
					</ul>


					<h1 style="font-size: 150%;">Visual embedding module</h1>
					<p>The visual embedding module is illustrated in the top row of Fig.1. On the basis of the original <a href="https://github.com/mpc001/Lipreading_using_Temporal_Convolutional_Networks">lipreading model</a>, we add three conformer blocks with 256 encoder dims, 4 attention heads, 32 conv kernel size and a 256-cell BLSTM to compute the visual embedding for speakers. The whole network can be regarded as visual voice activity detection (V-VAD) module. After pre-training the visual network as a V-VAD task, V-embeddings are equipped with capability that represent the states of speaking or silent. By feeding the embedding into the fully connected layers, we can get the probability of whether a speaker speaks in each frame. Combining the results of each speaker, we will get the initial diarization results.</p>

					<h1 style="font-size: 150%;">Audio embedding module</h1>
					<p>We firstly use the NARA-WPE method to dereverberate the original audio signals. Then, We extracted 40-dimensional FBANKs with 25 ms frame length and 10 ms frame shift. We extract the audio embedding from FBANKS through 4 layers 2D CNN. Then, a fully connected layer projects high dimensional CNNs output to low dimensional A-embeddings. Unlike visual network, we don’t pre-train audio network on any other tasks and directly optimize it with audio-visual decoding network.</p>

					<h1 style="font-size: 150%;">Speaker embedding module</h1>
					<p>To reduce the impact of the unreliable visual embedding, we use 100-dimensional i-vector extractor which was trained on Cn-celeb to get the i-vectors as the speaker embedding. We compute i-vectors through the oracle labels for each speaker in the training stage. And in the inference stage, we compute i-vectors through the initial diarization results from the visual embedding extraction module.</p>

					<h1 style="font-size: 150%;">Decoder module</h1>
					<p>We firstly repeat the three embeddings for different times to solve the problem of different frame shift between audio and video. Then, we combine them to get the total embedding. In the decoding block, we use 2-layer BLSTM with projection to further extract the features. Finally, we use a 1-layer BLSTM with projection and the fully connected layer to get the speech or non-speech probabilities for each speaker respectively. All of BLSTM layers contained 896 cells. In the post-processing stage, we first perform thresholding with the probabilities to produce a preliminary result and adopt the same approaches in <a href="https://ieeexplore.ieee.org/document/9747067">previous work</a>. Furthermore, <a href="https://github.com/desh2608/dover-lap">DOVER-Lap</a> is used to fuse the results of 6-channels audio.</p>
					
					<h1 style="font-size: 150%;">Decoder module</h1>
					<p>We firstly repeat the three embeddings for different times to solve the problem of different frame shift between audio and video. Then, we combine them to get the total embedding. In the decoding block, we use 2-layer BLSTM with projection to further extract the features. Finally, we use a 1-layer BLSTM with projection and the fully connected layer to get the speech or non-speech probabilities for each speaker respectively. All of BLSTM layers contained 896 cells. In the post-processing stage, we first perform thresholding with the probabilities to produce a preliminary result and adopt the same approaches in <a href="https://ieeexplore.ieee.org/document/9747067">previous work</a>. Furthermore, <a href="https://github.com/desh2608/dover-lap">DOVER-Lap</a> is used to fuse the results of 6-channels audio.</p>

					<h1 style="font-size: 150%;">Training process</h1>
					<p>First, we use the parameters of the pre-trained lipreading and train the V-VAD model with a learning rate of 10-4. Then, we freeze the visual network parameters and train the audio network and audio-visual decoding block on synchronized middle-field audio and video with a learning rate of 10<sup>−4</sup>. Finally,we unfreeze the visual network parameters and train the whole network jointly on synchronized middle-field audio and video with a learning rate of 10<sup>−5</sup>.</p>

					<h1 style="font-size: 150%;">Baseline Results</h1>
					<center>
						<figure style="padding:0px;border:0px; margin:10px">
						<img src="images/AVSD_baseline_result.png" style="width:500px;padding:0px;border:0" />
						</figure>
					</center>
					<p>We use Diarization error rate (DER) as the evaluation index. The above results are measured on the far-field audio and video. For the ASD system, we use the <a href="https://github.com/BUTSpeechFIT/VBx">VBx method</a>. For the VSD system, we use the result from the visual embedding module. And the AVSD system is our method. At the same time, this AVSD result also serves as the baseline result of the MISP2022 Challenge Track 1 in development set. The baseline results of the evaluation set will be published together with the data according to the schedule.</p>

					<h1 style="font-size: 150%;">Quick start</h1>
					<ul>
						<li><strong>Add file execution permissions</strong></li>
						<code>
							chmod +x -R /export/corpus/exp/av_diarization/misp2022_baseline/track1_AVSD/  # (Your code path) 
						</code>

						<li><strong>Data prepare</strong></li>
						<code>
							bash data_prepare.sh   # (Please change your file path in the script. Note that WPE is not necessary for training set) 
						</code>
						
						<li><strong>Setting kaldi</strong></li>
						<code>
							--- cmd.sh ---</br>
							export train_cmd=run.pl </br>
							export decode_cmd=run.pl</br>
							export cmd=run.pl</br>

							--- path.sh ---</br>
							export KALDI_ROOT=`pwd`/../../..        # Defining Kaldi root directory</br>
							#[ -f $KALDI_ROOT/tools/env.sh ] && . $KALDI_ROOT/tools/env.sh</br>
							Export PATH=$PWD/utils/:$KALDI_ROOT/tools/openfst/bin:$KALDI_ROOT/tools/irstlm/bin/:$PWD:$PATH</br>
							[ ! -f $KALDI_ROOT/tools/config/common_path.sh ] && echo >&2 "The standard file $KALDI_ROOT/tools/config/common_path.sh is not present -> Exit!" && exit 1</br>
							. $KALDI_ROOT/tools/config/common_path.sh</br>
							export LC_ALL=C
						</code>

						<li><strong>Pre-trained model</strong></li>
						<p>There are three pre-trained models.</p>
						<p>The conformer_v_sd_2.model and av_diarization_3.model are the pre-trained models of visual embedding module and audio-visual module respectively. You can choose whether to download them. Specifically, if you do not want to conduct the training process but directly decode, you can download these two models.</p>
						<p>Lipreading_LRW.pt is the pre-trained model of lipreading. The training process will use it. Therefore, if you want to conduct the training process, you must download this model.</p>
						<code>
							conformer_v_sd_2.model</br>
							av_diarization_3.model</br>
							lipreading_LRW.pt </br>		
						</code>
						<p></p>
						<p>The link of Google drive is: <a href="https://drive.google.com/drive/folders/1kh40NNBW84kODM0PrWvYLwDCjuqpKqj7?usp=sharing">https://drive.google.com/drive/folders/1kh40NNBW84kODM0PrWvYLwDCjuqpKqj7?usp=sharing</a></p>
						<p>Please put the downloaded model into model/pretrained/</p>
						

						<li><strong>Training</strong></li>
						<code>
							bash run.sh </br>
							# options: </br>
							&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;		--stage 1 </br>
							# change the number to start from different training stage </br>
							<p>Please change some file paths in the script to your own path.</p>
						</code>


						<li><strong>Decoding</strong></li>
						<p>If you have downloaded the pre-trained models, you can directly do the decoding process.</p>
						<code>
							bash run.sh </br>
							# options: </br>
							&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;		--stage 5 </br>
							# change the number to start from different training stage </br>
							(stage 5 ~ 9 are the decoding process using single channel audio; stage 10 ~ 11 are the decoding process fusing 6-channels audio)
						</code>
						<p>The lip ROI and ivector of the training set will be used in the decoding process. So you need to use stage2 of data_prepare.sh to extract the lip ROI of the training set, and use stage 3 of run.sh to extract the ivector of the training set. We have prepared the ivector of the training set. You can directly use it.</p>

						<h1 style="font-size: 150%;">Requirments</h1>
						<ul>
							<li>
								<p><strong>Kaldi</strong></p>
							</li>
							<li>
								<p><strong>pytorch</strong></p>
							</li>

							<li>
							<p><strong>Python Packages:</strong></br>
							nbformat</br>
							argparse</br>
							numpy</br>
							tqdm</br>
							opencv-python</br>
							einops</br>
							scipy</br>
							prefetch_generator</br>
							</p>
							</li>

							<li>
							<p><a href="https://github.com/fgnt/nara_wpe">Nare_wpe</a></p>
							</li>

							<li>
								<p><a href="https://github.com/desh2608/dover-lap">Dover</a></p>
							</li>

							<li>
								<p><a href="https://github.com/mpc001/Lipreading_using_Temporal_Convolutional_Networks">Lipreading using Temporal Convolutional Networks</a></p>
							</li>

							</ul>
					</ul>

					<h1 style="font-size: 150%;">Citation</h1>
					<p>If you find this code useful in your research, please consider to cite the following papers:</br>
						<font size="4">
							@inproceedings{he2022,</br>
							title={End-to-End Audio-Visual Neural Speaker Diarization},</br>
							author={He, Mao-kui and Du, Jun and Lee, Chin-Hui},</br>
							booktitle={Proc. Interspeech 2022},</br>
							pages={1461--1465},</br>
							year={2022}}
						</font>
					</p>
				</div>
			</section> -->

<!-- 				<section class="wrapper style1">
					<div class="container">
						<h1 style="font-size: 150%;">Task1 Software</h1>
						<p></p>
					</div>
				</section>

				<section class="wrapper style2">
					<div class="container">
						<h1 style="font-size: 150%;">......</h1>
					</div>
				</section>

				<section class="wrapper style1">
					<div class="container">
						<h1 style="font-size: 150%;">......</h1>
					</div>
				</section> -->

			<!-- Footer -->
				<div id="footer">
					<!-- Copyright -->
						<div class="copyright">
							<ul class="menu">
								<li>&copy; All rights reserved</li><li>E-mail: mispchallenge@gmail.com</li>
							</ul>
						</div>

				</div>

		</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.dropotron.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>
